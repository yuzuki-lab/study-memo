{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf42c33-2341-4d84-81a1-224b4e04f414",
   "metadata": {},
   "source": [
    "# cifar10用のresnetを用いたbyolの学習と評価\n",
    "参考code <br>\n",
    "BYOL: https://github.com/Spijkervet/BYOL<br>\n",
    "Resnet: https://github.com/kuangliu/pytorch-cifar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344a8c4-2736-4aeb-bc42-ee6a7d48ab23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### moduleの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "111bb12b-3e28-4cc3-9e3d-2d305b1bf955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "from functools import wraps\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, datasets\n",
    "\n",
    "# distributed training\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc345c5d-cf8e-4bc5-a2a7-36a969579165",
   "metadata": {
    "tags": []
   },
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f443f272-d43e-467e-8802-910b988d476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2020 Phil Wang\n",
    "https://github.com/lucidrains/byol-pytorch/\n",
    "\n",
    "Adjusted to de-couple for data loading, parallel training\n",
    "\"\"\"\n",
    "\n",
    "# helper functions\n",
    "\n",
    "\n",
    "def default(val, def_val):\n",
    "    return def_val if val is None else val\n",
    "\n",
    "\n",
    "def flatten(t):\n",
    "    return t.reshape(t.shape[0], -1)\n",
    "\n",
    "\n",
    "def singleton(cache_key):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            instance = getattr(self, cache_key)\n",
    "            if instance is not None:\n",
    "                return instance\n",
    "\n",
    "            instance = fn(self, *args, **kwargs)\n",
    "            setattr(self, cache_key, instance)\n",
    "            return instance\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return inner_fn\n",
    "\n",
    "\n",
    "# loss fn\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)\n",
    "\n",
    "\n",
    "# augmentation utils\n",
    "\n",
    "\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "# exponential moving average\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(\n",
    "        current_model.parameters(), ma_model.parameters()\n",
    "    ):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
    "\n",
    "\n",
    "# MLP class for projector and predictor\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, projection_size, hidden_size=4096):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, projection_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# a wrapper class for the base neural network\n",
    "# will manage the interception of the hidden layer output\n",
    "# and pipe it into the projecter and predictor nets\n",
    "\n",
    "\n",
    "class NetWrapper(nn.Module):\n",
    "    def __init__(self, net, projection_size, projection_hidden_size, layer=-2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.projector = None\n",
    "        self.projection_size = projection_size\n",
    "        self.projection_hidden_size = projection_hidden_size\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = flatten(output)\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f\"hidden layer ({self.layer}) not found\"\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    @singleton(\"projector\")\n",
    "    def _get_projector(self, hidden):\n",
    "        _, dim = hidden.shape\n",
    "        projector = MLP(dim, self.projection_size, self.projection_hidden_size)\n",
    "        return projector.to(hidden)\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f\"hidden layer {self.layer} never emitted an output\"\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        representation = self.get_representation(x)\n",
    "        projector = self._get_projector(representation)\n",
    "        projection = projector(representation)\n",
    "        return projection\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        image_size,\n",
    "        hidden_layer=-2,\n",
    "        projection_size=256,\n",
    "        projection_hidden_size=4096,\n",
    "        augment_fn=None,\n",
    "        moving_average_decay=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.online_encoder = NetWrapper(\n",
    "            net, projection_size, projection_hidden_size, layer=hidden_layer\n",
    "        )\n",
    "        self.target_encoder = None\n",
    "        self.target_ema_updater = EMA(moving_average_decay)\n",
    "\n",
    "        self.online_predictor = MLP(\n",
    "            projection_size, projection_size, projection_hidden_size\n",
    "        )\n",
    "\n",
    "        # send a mock image tensor to instantiate singleton parameters\n",
    "        self.forward(torch.randn(2, 3, image_size, image_size), torch.randn(2, 3, image_size, image_size))\n",
    "\n",
    "    @singleton(\"target_encoder\")\n",
    "    def _get_target_encoder(self):\n",
    "        target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        return target_encoder\n",
    "\n",
    "    def reset_moving_average(self):\n",
    "        del self.target_encoder\n",
    "        self.target_encoder = None\n",
    "\n",
    "    def update_moving_average(self):\n",
    "        assert (\n",
    "            self.target_encoder is not None\n",
    "        ), \"target encoder has not been created yet\"\n",
    "        update_moving_average(\n",
    "            self.target_ema_updater, self.target_encoder, self.online_encoder\n",
    "        )\n",
    "\n",
    "    def forward(self, image_one, image_two):\n",
    "        online_proj_one = self.online_encoder(image_one)\n",
    "        online_proj_two = self.online_encoder(image_two)\n",
    "\n",
    "        online_pred_one = self.online_predictor(online_proj_one)\n",
    "        online_pred_two = self.online_predictor(online_proj_two)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_encoder = self._get_target_encoder()\n",
    "            target_proj_one = target_encoder(image_one)\n",
    "            target_proj_two = target_encoder(image_two)\n",
    "\n",
    "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
    "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
    "\n",
    "        loss = loss_one + loss_two\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b6e12f-46ab-4e3f-90e9-034bfb81fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):#################\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(4)###########################\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        #out = F.avg_pool2d(out, 4)#################\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b765c9-7418-4ecf-b891-cf39a7827bc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### data拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4145575a-cdd8-4a94-8ec5-75a392a7aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformsSimCLR:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module that transforms any given data example randomly \n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        s = 1\n",
    "        color_jitter = torchvision.transforms.ColorJitter(\n",
    "            0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s\n",
    "        )\n",
    "        self.train_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.RandomResizedCrop(size=size),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
    "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
    "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.test_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(size=size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x), self.train_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e44aa5-cdc5-4e48-93af-a8ba6a9b1a63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165becb9-d449-45e3-9cfc-cc6412dc3873",
   "metadata": {
    "tags": []
   },
   "source": [
    "### parameter設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc73d921-c2fd-4f17-bf96-8924df2bfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--image_size\", default=32, type=int, help=\"Image size\")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\", default=3e-4, type=float, help=\"Initial learning rate.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=192, type=int, help=\"Batch size for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\", default=1000, type=int, help=\"Number of epochs to train for.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resnet_version\", default=\"resnet18\", type=str, help=\"ResNet version.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_epochs\",\n",
    "    default=50,\n",
    "    type=int,\n",
    "    help=\"Number of epochs between checkpoints/summaries.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_dir\",\n",
    "    default=\"./datasets\",\n",
    "    type=str,\n",
    "    help=\"Directory where dataset is stored.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    default=8,\n",
    "    type=int,\n",
    "    help=\"Number of data loading workers (caution with nodes!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--nodes\", default=1, type=int, help=\"Number of nodes\",\n",
    ")\n",
    "parser.add_argument(\"--gpus\", default=1, type=int, help=\"number of gpus per node\")\n",
    "parser.add_argument(\"--nr\", default=0, type=int, help=\"ranking within the nodes\")\n",
    "parser.add_argument(\"--cuda\", default=\"cuda:0\", type=str, help=\"cuda number\")\n",
    "parser.add_argument(\"--seed\", default=0, type=int, help=\"number of seed\")\n",
    "\n",
    "# colab work-around\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12a846-db86-4b06-a7cf-b406adcd10ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6f892a-7db1-45b4-99ee-9accc9a53756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    args.dataset_dir,\n",
    "    download=True,\n",
    "    transform=TransformsSimCLR(size=args.image_size), # paper 224\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e5d52-5dad-4b39-bdb7-794e4899f52b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64b1c8-9d4e-4765-b8fd-9359183efc78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/260]:\tLoss: 3.961852788925171\n",
      "Step [1/260]:\tLoss: 2.469606637954712\n",
      "Step [2/260]:\tLoss: 2.0464189052581787\n",
      "Step [3/260]:\tLoss: 1.9024022817611694\n",
      "Step [4/260]:\tLoss: 1.8080511093139648\n",
      "Step [5/260]:\tLoss: 1.8205604553222656\n",
      "Step [6/260]:\tLoss: 1.7964218854904175\n",
      "Step [7/260]:\tLoss: 1.7622851133346558\n",
      "Step [8/260]:\tLoss: 1.7911841869354248\n",
      "Step [9/260]:\tLoss: 1.7791122198104858\n",
      "Step [10/260]:\tLoss: 1.78219735622406\n",
      "Step [11/260]:\tLoss: 1.8013203144073486\n",
      "Step [12/260]:\tLoss: 1.8329315185546875\n",
      "Step [13/260]:\tLoss: 1.8023478984832764\n",
      "Step [14/260]:\tLoss: 1.8147344589233398\n",
      "Step [15/260]:\tLoss: 1.8062710762023926\n",
      "Step [16/260]:\tLoss: 1.809901237487793\n",
      "Step [17/260]:\tLoss: 1.7953367233276367\n",
      "Step [18/260]:\tLoss: 1.8214950561523438\n",
      "Step [19/260]:\tLoss: 1.8240258693695068\n",
      "Step [20/260]:\tLoss: 1.8194353580474854\n",
      "Step [21/260]:\tLoss: 1.810143232345581\n",
      "Step [22/260]:\tLoss: 1.8085260391235352\n",
      "Step [23/260]:\tLoss: 1.8486522436141968\n",
      "Step [24/260]:\tLoss: 1.846562385559082\n",
      "Step [25/260]:\tLoss: 1.7789018154144287\n",
      "Step [26/260]:\tLoss: 1.840490698814392\n",
      "Step [27/260]:\tLoss: 1.8036892414093018\n",
      "Step [28/260]:\tLoss: 1.8508286476135254\n",
      "Step [29/260]:\tLoss: 1.8644651174545288\n",
      "Step [30/260]:\tLoss: 1.8872759342193604\n",
      "Step [31/260]:\tLoss: 1.83324134349823\n",
      "Step [32/260]:\tLoss: 1.7938860654830933\n",
      "Step [33/260]:\tLoss: 1.7963823080062866\n",
      "Step [34/260]:\tLoss: 1.7387875318527222\n",
      "Step [35/260]:\tLoss: 1.7694114446640015\n",
      "Step [36/260]:\tLoss: 1.8465287685394287\n",
      "Step [37/260]:\tLoss: 1.8180406093597412\n",
      "Step [38/260]:\tLoss: 1.7664449214935303\n",
      "Step [39/260]:\tLoss: 1.795651912689209\n",
      "Step [40/260]:\tLoss: 1.7311748266220093\n",
      "Step [41/260]:\tLoss: 1.7266757488250732\n",
      "Step [42/260]:\tLoss: 1.7586244344711304\n",
      "Step [43/260]:\tLoss: 1.7972825765609741\n",
      "Step [44/260]:\tLoss: 1.8482627868652344\n",
      "Step [45/260]:\tLoss: 1.7952226400375366\n",
      "Step [46/260]:\tLoss: 1.7047580480575562\n",
      "Step [47/260]:\tLoss: 1.8052719831466675\n",
      "Step [48/260]:\tLoss: 1.6349581480026245\n",
      "Step [49/260]:\tLoss: 1.7755922079086304\n",
      "Step [50/260]:\tLoss: 1.7375677824020386\n",
      "Step [51/260]:\tLoss: 1.7566719055175781\n",
      "Step [52/260]:\tLoss: 1.7499898672103882\n",
      "Step [53/260]:\tLoss: 1.7491201162338257\n",
      "Step [54/260]:\tLoss: 1.7128452062606812\n",
      "Step [55/260]:\tLoss: 1.6989610195159912\n",
      "Step [56/260]:\tLoss: 1.6735649108886719\n",
      "Step [57/260]:\tLoss: 1.729761004447937\n",
      "Step [58/260]:\tLoss: 1.7424004077911377\n",
      "Step [59/260]:\tLoss: 1.7098721265792847\n",
      "Step [60/260]:\tLoss: 1.6080248355865479\n",
      "Step [61/260]:\tLoss: 1.7062910795211792\n",
      "Step [62/260]:\tLoss: 1.695210337638855\n",
      "Step [63/260]:\tLoss: 1.706385612487793\n",
      "Step [64/260]:\tLoss: 1.7205079793930054\n",
      "Step [65/260]:\tLoss: 1.6898750066757202\n",
      "Step [66/260]:\tLoss: 1.6989362239837646\n",
      "Step [67/260]:\tLoss: 1.7037041187286377\n",
      "Step [68/260]:\tLoss: 1.6529855728149414\n",
      "Step [69/260]:\tLoss: 1.7824020385742188\n",
      "Step [70/260]:\tLoss: 1.667080283164978\n",
      "Step [71/260]:\tLoss: 1.6868572235107422\n",
      "Step [72/260]:\tLoss: 1.547781229019165\n",
      "Step [73/260]:\tLoss: 1.6401124000549316\n",
      "Step [74/260]:\tLoss: 1.7151665687561035\n",
      "Step [75/260]:\tLoss: 1.70712411403656\n",
      "Step [76/260]:\tLoss: 1.7192742824554443\n",
      "Step [77/260]:\tLoss: 1.695511817932129\n",
      "Step [78/260]:\tLoss: 1.5972568988800049\n",
      "Step [79/260]:\tLoss: 1.6735525131225586\n",
      "Step [80/260]:\tLoss: 1.6698116064071655\n",
      "Step [81/260]:\tLoss: 1.6790498495101929\n",
      "Step [82/260]:\tLoss: 1.646706461906433\n",
      "Step [83/260]:\tLoss: 1.631113886833191\n",
      "Step [84/260]:\tLoss: 1.582909345626831\n",
      "Step [85/260]:\tLoss: 1.6957197189331055\n",
      "Step [86/260]:\tLoss: 1.5650460720062256\n",
      "Step [87/260]:\tLoss: 1.6345270872116089\n",
      "Step [88/260]:\tLoss: 1.570707082748413\n",
      "Step [89/260]:\tLoss: 1.643803596496582\n",
      "Step [90/260]:\tLoss: 1.625248670578003\n",
      "Step [91/260]:\tLoss: 1.5418307781219482\n",
      "Step [92/260]:\tLoss: 1.5232595205307007\n",
      "Step [93/260]:\tLoss: 1.6393303871154785\n",
      "Step [94/260]:\tLoss: 1.5942628383636475\n",
      "Step [95/260]:\tLoss: 1.5723295211791992\n",
      "Step [96/260]:\tLoss: 1.4706171751022339\n",
      "Step [97/260]:\tLoss: 1.5518465042114258\n",
      "Step [98/260]:\tLoss: 1.4860460758209229\n",
      "Step [99/260]:\tLoss: 1.5248903036117554\n",
      "Step [100/260]:\tLoss: 1.5606873035430908\n",
      "Step [101/260]:\tLoss: 1.5715259313583374\n",
      "Step [102/260]:\tLoss: 1.5511603355407715\n",
      "Step [103/260]:\tLoss: 1.4942245483398438\n",
      "Step [104/260]:\tLoss: 1.4947221279144287\n",
      "Step [105/260]:\tLoss: 1.5064208507537842\n",
      "Step [106/260]:\tLoss: 1.5064877271652222\n",
      "Step [107/260]:\tLoss: 1.5046401023864746\n",
      "Step [108/260]:\tLoss: 1.5150518417358398\n",
      "Step [109/260]:\tLoss: 1.509684443473816\n",
      "Step [110/260]:\tLoss: 1.3412625789642334\n",
      "Step [111/260]:\tLoss: 1.4085800647735596\n",
      "Step [112/260]:\tLoss: 1.499190330505371\n",
      "Step [113/260]:\tLoss: 1.4486116170883179\n",
      "Step [114/260]:\tLoss: 1.481522560119629\n",
      "Step [115/260]:\tLoss: 1.4398503303527832\n",
      "Step [116/260]:\tLoss: 1.476552963256836\n",
      "Step [117/260]:\tLoss: 1.6159371137619019\n",
      "Step [118/260]:\tLoss: 1.4534416198730469\n",
      "Step [119/260]:\tLoss: 1.4264581203460693\n",
      "Step [120/260]:\tLoss: 1.5135483741760254\n",
      "Step [121/260]:\tLoss: 1.472184658050537\n",
      "Step [122/260]:\tLoss: 1.3554189205169678\n",
      "Step [123/260]:\tLoss: 1.505727767944336\n",
      "Step [124/260]:\tLoss: 1.4337776899337769\n",
      "Step [125/260]:\tLoss: 1.4611114263534546\n",
      "Step [126/260]:\tLoss: 1.4155232906341553\n",
      "Step [127/260]:\tLoss: 1.403374433517456\n",
      "Step [128/260]:\tLoss: 1.471388339996338\n",
      "Step [129/260]:\tLoss: 1.4412264823913574\n",
      "Step [130/260]:\tLoss: 1.4794511795043945\n",
      "Step [131/260]:\tLoss: 1.4750525951385498\n",
      "Step [132/260]:\tLoss: 1.3979262113571167\n",
      "Step [133/260]:\tLoss: 1.4567950963974\n",
      "Step [134/260]:\tLoss: 1.4002189636230469\n",
      "Step [135/260]:\tLoss: 1.4375255107879639\n",
      "Step [136/260]:\tLoss: 1.3737068176269531\n",
      "Step [137/260]:\tLoss: 1.3516032695770264\n",
      "Step [138/260]:\tLoss: 1.3081557750701904\n",
      "Step [139/260]:\tLoss: 1.3328566551208496\n",
      "Step [140/260]:\tLoss: 1.3070430755615234\n",
      "Step [141/260]:\tLoss: 1.4369548559188843\n",
      "Step [142/260]:\tLoss: 1.3643884658813477\n",
      "Step [143/260]:\tLoss: 1.314338207244873\n",
      "Step [144/260]:\tLoss: 1.450662612915039\n",
      "Step [145/260]:\tLoss: 1.3271045684814453\n",
      "Step [146/260]:\tLoss: 1.233102798461914\n",
      "Step [147/260]:\tLoss: 1.287240743637085\n",
      "Step [148/260]:\tLoss: 1.4004857540130615\n",
      "Step [149/260]:\tLoss: 1.3573497533798218\n",
      "Step [150/260]:\tLoss: 1.2753403186798096\n",
      "Step [151/260]:\tLoss: 1.3294929265975952\n",
      "Step [152/260]:\tLoss: 1.4099630117416382\n",
      "Step [153/260]:\tLoss: 1.2436935901641846\n",
      "Step [154/260]:\tLoss: 1.3715442419052124\n",
      "Step [155/260]:\tLoss: 1.3039213418960571\n",
      "Step [156/260]:\tLoss: 1.4407933950424194\n",
      "Step [157/260]:\tLoss: 1.4252357482910156\n",
      "Step [158/260]:\tLoss: 1.2042251825332642\n",
      "Step [159/260]:\tLoss: 1.3055952787399292\n",
      "Step [160/260]:\tLoss: 1.2919373512268066\n",
      "Step [161/260]:\tLoss: 1.2452138662338257\n",
      "Step [162/260]:\tLoss: 1.3249026536941528\n",
      "Step [163/260]:\tLoss: 1.1682924032211304\n",
      "Step [164/260]:\tLoss: 1.3444671630859375\n",
      "Step [165/260]:\tLoss: 1.2415580749511719\n",
      "Step [166/260]:\tLoss: 1.1455150842666626\n",
      "Step [167/260]:\tLoss: 1.2236499786376953\n",
      "Step [168/260]:\tLoss: 1.1962344646453857\n",
      "Step [169/260]:\tLoss: 1.2913422584533691\n",
      "Step [170/260]:\tLoss: 1.21773099899292\n",
      "Step [171/260]:\tLoss: 1.284023642539978\n",
      "Step [172/260]:\tLoss: 1.2586438655853271\n",
      "Step [173/260]:\tLoss: 1.1531691551208496\n",
      "Step [174/260]:\tLoss: 1.2443636655807495\n",
      "Step [175/260]:\tLoss: 1.2257614135742188\n",
      "Step [176/260]:\tLoss: 1.2767376899719238\n",
      "Step [177/260]:\tLoss: 1.314562201499939\n",
      "Step [178/260]:\tLoss: 1.1742101907730103\n",
      "Step [179/260]:\tLoss: 1.2169803380966187\n",
      "Step [180/260]:\tLoss: 1.2440298795700073\n",
      "Step [181/260]:\tLoss: 1.267185091972351\n",
      "Step [182/260]:\tLoss: 1.2487380504608154\n",
      "Step [183/260]:\tLoss: 1.201247215270996\n",
      "Step [184/260]:\tLoss: 1.1331613063812256\n",
      "Step [185/260]:\tLoss: 1.172425627708435\n",
      "Step [186/260]:\tLoss: 1.213135838508606\n",
      "Step [187/260]:\tLoss: 1.3172802925109863\n",
      "Step [188/260]:\tLoss: 1.1527583599090576\n",
      "Step [189/260]:\tLoss: 1.270059585571289\n",
      "Step [190/260]:\tLoss: 1.1920607089996338\n",
      "Step [191/260]:\tLoss: 1.237369418144226\n",
      "Step [192/260]:\tLoss: 1.1309623718261719\n",
      "Step [193/260]:\tLoss: 1.2484210729599\n",
      "Step [194/260]:\tLoss: 1.1485062837600708\n",
      "Step [195/260]:\tLoss: 1.291646957397461\n",
      "Step [196/260]:\tLoss: 1.08456289768219\n",
      "Step [197/260]:\tLoss: 1.2693278789520264\n",
      "Step [198/260]:\tLoss: 1.2245051860809326\n",
      "Step [199/260]:\tLoss: 1.2356035709381104\n",
      "Step [200/260]:\tLoss: 1.177475094795227\n",
      "Step [201/260]:\tLoss: 1.2202227115631104\n",
      "Step [202/260]:\tLoss: 1.2342084646224976\n",
      "Step [203/260]:\tLoss: 1.244538426399231\n",
      "Step [204/260]:\tLoss: 1.2255096435546875\n",
      "Step [205/260]:\tLoss: 1.2066242694854736\n",
      "Step [206/260]:\tLoss: 1.2737212181091309\n",
      "Step [207/260]:\tLoss: 1.2831228971481323\n",
      "Step [208/260]:\tLoss: 1.233837604522705\n",
      "Step [209/260]:\tLoss: 1.26230788230896\n",
      "Step [210/260]:\tLoss: 1.2471904754638672\n",
      "Step [211/260]:\tLoss: 1.1077382564544678\n",
      "Step [212/260]:\tLoss: 1.2659177780151367\n",
      "Step [213/260]:\tLoss: 1.2078685760498047\n",
      "Step [214/260]:\tLoss: 1.307847261428833\n",
      "Step [215/260]:\tLoss: 1.2615127563476562\n",
      "Step [216/260]:\tLoss: 1.2004374265670776\n",
      "Step [217/260]:\tLoss: 1.278260588645935\n",
      "Step [218/260]:\tLoss: 1.2056028842926025\n",
      "Step [219/260]:\tLoss: 1.1272549629211426\n",
      "Step [220/260]:\tLoss: 1.1614879369735718\n",
      "Step [221/260]:\tLoss: 1.193533182144165\n",
      "Step [222/260]:\tLoss: 1.1013344526290894\n",
      "Step [223/260]:\tLoss: 1.1169335842132568\n",
      "Step [224/260]:\tLoss: 1.1283609867095947\n",
      "Step [225/260]:\tLoss: 1.217315912246704\n",
      "Step [226/260]:\tLoss: 1.171140193939209\n",
      "Step [227/260]:\tLoss: 1.0798845291137695\n",
      "Step [228/260]:\tLoss: 1.2097265720367432\n",
      "Step [229/260]:\tLoss: 1.0676155090332031\n",
      "Step [230/260]:\tLoss: 1.1976561546325684\n",
      "Step [231/260]:\tLoss: 1.1518093347549438\n",
      "Step [232/260]:\tLoss: 1.034565806388855\n",
      "Step [233/260]:\tLoss: 1.1200647354125977\n",
      "Step [234/260]:\tLoss: 1.1619282960891724\n",
      "Step [235/260]:\tLoss: 1.1320618391036987\n",
      "Step [236/260]:\tLoss: 1.1010297536849976\n",
      "Step [237/260]:\tLoss: 1.1288824081420898\n",
      "Step [238/260]:\tLoss: 1.136064052581787\n",
      "Step [239/260]:\tLoss: 1.185420274734497\n",
      "Step [240/260]:\tLoss: 1.134155035018921\n",
      "Step [241/260]:\tLoss: 0.9779559373855591\n",
      "Step [242/260]:\tLoss: 1.209284782409668\n",
      "Step [243/260]:\tLoss: 1.1167815923690796\n",
      "Step [244/260]:\tLoss: 1.189504623413086\n",
      "Step [245/260]:\tLoss: 1.1184115409851074\n",
      "Step [246/260]:\tLoss: 1.2359092235565186\n",
      "Step [247/260]:\tLoss: 1.0584778785705566\n",
      "Step [248/260]:\tLoss: 1.2593731880187988\n",
      "Step [249/260]:\tLoss: 1.2828444242477417\n",
      "Step [250/260]:\tLoss: 1.1954071521759033\n",
      "Step [251/260]:\tLoss: 1.0646119117736816\n",
      "Step [252/260]:\tLoss: 1.1610431671142578\n",
      "Step [253/260]:\tLoss: 1.1512138843536377\n",
      "Step [254/260]:\tLoss: 1.1275489330291748\n",
      "Step [255/260]:\tLoss: 1.2360787391662598\n",
      "Step [256/260]:\tLoss: 1.2325949668884277\n",
      "Step [257/260]:\tLoss: 1.0435928106307983\n",
      "Step [258/260]:\tLoss: 1.107694387435913\n",
      "Step [259/260]:\tLoss: 1.2073640823364258\n",
      "Epoch [0/1000]: Loss/train: 1.463571868951504\ttime:33.39938235282898\n",
      "Saving model at epoch 0\n",
      "Step [0/260]:\tLoss: 1.1900036334991455\n",
      "Step [1/260]:\tLoss: 1.0930249691009521\n",
      "Step [2/260]:\tLoss: 1.0579923391342163\n",
      "Step [3/260]:\tLoss: 1.0391724109649658\n",
      "Step [4/260]:\tLoss: 1.1432061195373535\n",
      "Step [5/260]:\tLoss: 1.053007960319519\n",
      "Step [6/260]:\tLoss: 1.10550057888031\n",
      "Step [7/260]:\tLoss: 1.0799607038497925\n",
      "Step [8/260]:\tLoss: 1.0828081369400024\n",
      "Step [9/260]:\tLoss: 1.131390929222107\n",
      "Step [10/260]:\tLoss: 1.2005455493927002\n",
      "Step [11/260]:\tLoss: 1.1040295362472534\n",
      "Step [12/260]:\tLoss: 1.1724978685379028\n",
      "Step [13/260]:\tLoss: 1.0920813083648682\n",
      "Step [14/260]:\tLoss: 1.1293097734451294\n",
      "Step [15/260]:\tLoss: 1.1182349920272827\n",
      "Step [16/260]:\tLoss: 1.0545334815979004\n",
      "Step [17/260]:\tLoss: 1.107528567314148\n",
      "Step [18/260]:\tLoss: 1.0973684787750244\n",
      "Step [19/260]:\tLoss: 1.108583927154541\n",
      "Step [20/260]:\tLoss: 1.0544055700302124\n",
      "Step [21/260]:\tLoss: 1.0915982723236084\n",
      "Step [22/260]:\tLoss: 1.0714614391326904\n",
      "Step [23/260]:\tLoss: 1.1524312496185303\n",
      "Step [24/260]:\tLoss: 1.0182552337646484\n",
      "Step [25/260]:\tLoss: 1.109610915184021\n",
      "Step [26/260]:\tLoss: 1.1650294065475464\n",
      "Step [27/260]:\tLoss: 1.0726089477539062\n",
      "Step [28/260]:\tLoss: 1.0831515789031982\n",
      "Step [29/260]:\tLoss: 0.9614503383636475\n",
      "Step [30/260]:\tLoss: 0.997970461845398\n",
      "Step [31/260]:\tLoss: 1.0832165479660034\n",
      "Step [32/260]:\tLoss: 1.0922749042510986\n",
      "Step [33/260]:\tLoss: 1.0894880294799805\n",
      "Step [34/260]:\tLoss: 1.1276788711547852\n",
      "Step [35/260]:\tLoss: 1.003105640411377\n",
      "Step [36/260]:\tLoss: 1.072505235671997\n",
      "Step [37/260]:\tLoss: 1.0220859050750732\n",
      "Step [38/260]:\tLoss: 1.076430320739746\n",
      "Step [39/260]:\tLoss: 1.014883279800415\n",
      "Step [40/260]:\tLoss: 1.1507794857025146\n",
      "Step [41/260]:\tLoss: 1.0571463108062744\n",
      "Step [42/260]:\tLoss: 1.0642080307006836\n",
      "Step [43/260]:\tLoss: 1.115360975265503\n",
      "Step [44/260]:\tLoss: 0.9699012637138367\n",
      "Step [45/260]:\tLoss: 1.051720142364502\n",
      "Step [46/260]:\tLoss: 1.0224398374557495\n",
      "Step [47/260]:\tLoss: 1.0107611417770386\n",
      "Step [48/260]:\tLoss: 1.0758687257766724\n",
      "Step [49/260]:\tLoss: 1.156831979751587\n",
      "Step [50/260]:\tLoss: 0.9360876083374023\n",
      "Step [51/260]:\tLoss: 1.0615805387496948\n",
      "Step [52/260]:\tLoss: 0.9097931385040283\n",
      "Step [53/260]:\tLoss: 1.1191819906234741\n",
      "Step [54/260]:\tLoss: 1.0118821859359741\n",
      "Step [55/260]:\tLoss: 1.0010931491851807\n",
      "Step [56/260]:\tLoss: 1.0505590438842773\n",
      "Step [57/260]:\tLoss: 1.0605219602584839\n",
      "Step [58/260]:\tLoss: 0.929924488067627\n",
      "Step [59/260]:\tLoss: 1.0142009258270264\n",
      "Step [60/260]:\tLoss: 1.1156163215637207\n",
      "Step [61/260]:\tLoss: 1.0018887519836426\n",
      "Step [62/260]:\tLoss: 1.0307689905166626\n",
      "Step [63/260]:\tLoss: 0.9242236614227295\n",
      "Step [64/260]:\tLoss: 1.0210884809494019\n",
      "Step [65/260]:\tLoss: 1.1231530904769897\n",
      "Step [66/260]:\tLoss: 1.023782730102539\n",
      "Step [67/260]:\tLoss: 1.0666074752807617\n",
      "Step [68/260]:\tLoss: 0.8985335230827332\n",
      "Step [69/260]:\tLoss: 0.9959980845451355\n",
      "Step [70/260]:\tLoss: 0.9773826599121094\n",
      "Step [71/260]:\tLoss: 0.9351142644882202\n",
      "Step [72/260]:\tLoss: 1.053889513015747\n",
      "Step [73/260]:\tLoss: 0.840450644493103\n",
      "Step [74/260]:\tLoss: 0.9914721250534058\n",
      "Step [75/260]:\tLoss: 0.9565913081169128\n",
      "Step [76/260]:\tLoss: 1.0831212997436523\n",
      "Step [77/260]:\tLoss: 1.0458040237426758\n",
      "Step [78/260]:\tLoss: 1.1196794509887695\n",
      "Step [79/260]:\tLoss: 0.9963390827178955\n",
      "Step [80/260]:\tLoss: 0.9743008613586426\n",
      "Step [81/260]:\tLoss: 0.9448662996292114\n",
      "Step [82/260]:\tLoss: 1.012679100036621\n",
      "Step [83/260]:\tLoss: 0.8682945966720581\n",
      "Step [84/260]:\tLoss: 1.0157074928283691\n",
      "Step [85/260]:\tLoss: 1.001101016998291\n",
      "Step [86/260]:\tLoss: 1.046452283859253\n",
      "Step [87/260]:\tLoss: 0.9417282938957214\n",
      "Step [88/260]:\tLoss: 0.9565342664718628\n",
      "Step [89/260]:\tLoss: 1.0367276668548584\n",
      "Step [90/260]:\tLoss: 0.9471851587295532\n",
      "Step [91/260]:\tLoss: 0.9853854179382324\n",
      "Step [92/260]:\tLoss: 0.9949066638946533\n",
      "Step [93/260]:\tLoss: 0.9400169849395752\n",
      "Step [94/260]:\tLoss: 1.0874618291854858\n",
      "Step [95/260]:\tLoss: 0.9839774966239929\n",
      "Step [96/260]:\tLoss: 1.0320844650268555\n",
      "Step [97/260]:\tLoss: 0.9526554942131042\n",
      "Step [98/260]:\tLoss: 0.8907414674758911\n",
      "Step [99/260]:\tLoss: 0.9281873106956482\n",
      "Step [100/260]:\tLoss: 1.0674904584884644\n",
      "Step [101/260]:\tLoss: 1.0394971370697021\n",
      "Step [102/260]:\tLoss: 0.9238346815109253\n",
      "Step [103/260]:\tLoss: 0.9687871932983398\n",
      "Step [104/260]:\tLoss: 0.9166671633720398\n",
      "Step [105/260]:\tLoss: 1.065413475036621\n",
      "Step [106/260]:\tLoss: 0.864497184753418\n",
      "Step [107/260]:\tLoss: 1.0222034454345703\n",
      "Step [108/260]:\tLoss: 0.9523646831512451\n",
      "Step [109/260]:\tLoss: 1.0356769561767578\n",
      "Step [110/260]:\tLoss: 1.1048777103424072\n",
      "Step [111/260]:\tLoss: 0.9553288221359253\n",
      "Step [112/260]:\tLoss: 0.9816765785217285\n",
      "Step [113/260]:\tLoss: 0.9246470332145691\n",
      "Step [114/260]:\tLoss: 0.99223792552948\n",
      "Step [115/260]:\tLoss: 0.8837894201278687\n",
      "Step [116/260]:\tLoss: 0.8323359489440918\n",
      "Step [117/260]:\tLoss: 0.8796490430831909\n",
      "Step [118/260]:\tLoss: 0.8961291313171387\n",
      "Step [119/260]:\tLoss: 0.9306563138961792\n",
      "Step [120/260]:\tLoss: 0.886999785900116\n",
      "Step [121/260]:\tLoss: 1.0397381782531738\n",
      "Step [122/260]:\tLoss: 0.9118337631225586\n",
      "Step [123/260]:\tLoss: 1.0014448165893555\n",
      "Step [124/260]:\tLoss: 0.8441605567932129\n",
      "Step [125/260]:\tLoss: 0.9440914988517761\n",
      "Step [126/260]:\tLoss: 0.9442064762115479\n",
      "Step [127/260]:\tLoss: 0.9695439338684082\n",
      "Step [128/260]:\tLoss: 0.8486819267272949\n",
      "Step [129/260]:\tLoss: 1.0691711902618408\n",
      "Step [130/260]:\tLoss: 1.0315392017364502\n",
      "Step [131/260]:\tLoss: 0.9159318804740906\n",
      "Step [132/260]:\tLoss: 0.9983605742454529\n",
      "Step [133/260]:\tLoss: 1.0100796222686768\n",
      "Step [134/260]:\tLoss: 0.9141899943351746\n",
      "Step [135/260]:\tLoss: 1.0620293617248535\n",
      "Step [136/260]:\tLoss: 0.9091247320175171\n",
      "Step [137/260]:\tLoss: 0.9407860636711121\n",
      "Step [138/260]:\tLoss: 0.8920985460281372\n",
      "Step [139/260]:\tLoss: 0.9541563987731934\n",
      "Step [140/260]:\tLoss: 1.0169637203216553\n",
      "Step [141/260]:\tLoss: 1.0869879722595215\n",
      "Step [142/260]:\tLoss: 0.9451922178268433\n",
      "Step [143/260]:\tLoss: 0.8427042961120605\n",
      "Step [144/260]:\tLoss: 0.9904237985610962\n",
      "Step [145/260]:\tLoss: 0.9296994209289551\n",
      "Step [146/260]:\tLoss: 0.9547216892242432\n",
      "Step [147/260]:\tLoss: 0.9632973670959473\n",
      "Step [148/260]:\tLoss: 1.0016634464263916\n",
      "Step [149/260]:\tLoss: 1.0175621509552002\n",
      "Step [150/260]:\tLoss: 0.9400522708892822\n",
      "Step [151/260]:\tLoss: 0.9265062212944031\n",
      "Step [152/260]:\tLoss: 0.9604185819625854\n",
      "Step [153/260]:\tLoss: 1.0038723945617676\n",
      "Step [154/260]:\tLoss: 1.0237082242965698\n",
      "Step [155/260]:\tLoss: 0.9533804655075073\n",
      "Step [156/260]:\tLoss: 0.9692243337631226\n",
      "Step [157/260]:\tLoss: 0.9909294843673706\n",
      "Step [158/260]:\tLoss: 0.8496917486190796\n",
      "Step [159/260]:\tLoss: 0.9355327486991882\n",
      "Step [160/260]:\tLoss: 0.9390789866447449\n",
      "Step [161/260]:\tLoss: 0.812746524810791\n",
      "Step [162/260]:\tLoss: 0.8903504610061646\n",
      "Step [163/260]:\tLoss: 0.8782351016998291\n",
      "Step [164/260]:\tLoss: 0.9251861572265625\n",
      "Step [165/260]:\tLoss: 1.0231091976165771\n",
      "Step [166/260]:\tLoss: 0.806557834148407\n",
      "Step [167/260]:\tLoss: 0.9401254653930664\n",
      "Step [168/260]:\tLoss: 0.7329262495040894\n",
      "Step [169/260]:\tLoss: 1.0561500787734985\n",
      "Step [170/260]:\tLoss: 0.8772128820419312\n",
      "Step [171/260]:\tLoss: 0.9831538200378418\n",
      "Step [172/260]:\tLoss: 0.7759246826171875\n",
      "Step [173/260]:\tLoss: 0.9868966937065125\n",
      "Step [174/260]:\tLoss: 0.9240037798881531\n",
      "Step [175/260]:\tLoss: 0.9490298628807068\n",
      "Step [176/260]:\tLoss: 1.0332355499267578\n",
      "Step [177/260]:\tLoss: 0.9681677222251892\n",
      "Step [178/260]:\tLoss: 0.9169447422027588\n",
      "Step [179/260]:\tLoss: 0.8985887765884399\n",
      "Step [180/260]:\tLoss: 0.9529964327812195\n",
      "Step [181/260]:\tLoss: 0.9020149111747742\n",
      "Step [182/260]:\tLoss: 0.9904513359069824\n",
      "Step [183/260]:\tLoss: 0.8513169288635254\n",
      "Step [184/260]:\tLoss: 0.8503670692443848\n",
      "Step [185/260]:\tLoss: 0.908585786819458\n",
      "Step [186/260]:\tLoss: 0.8730265498161316\n",
      "Step [187/260]:\tLoss: 0.8904852867126465\n",
      "Step [188/260]:\tLoss: 0.9052815437316895\n",
      "Step [189/260]:\tLoss: 0.851564884185791\n",
      "Step [190/260]:\tLoss: 0.986842155456543\n",
      "Step [191/260]:\tLoss: 0.7965229749679565\n",
      "Step [192/260]:\tLoss: 0.9162626266479492\n",
      "Step [193/260]:\tLoss: 0.8722435832023621\n",
      "Step [194/260]:\tLoss: 0.9131109118461609\n",
      "Step [195/260]:\tLoss: 0.7943205237388611\n",
      "Step [196/260]:\tLoss: 0.9807180166244507\n",
      "Step [197/260]:\tLoss: 0.9852554798126221\n",
      "Step [198/260]:\tLoss: 1.0394717454910278\n",
      "Step [199/260]:\tLoss: 0.8637843132019043\n",
      "Step [200/260]:\tLoss: 0.864704966545105\n",
      "Step [201/260]:\tLoss: 0.8049961924552917\n",
      "Step [202/260]:\tLoss: 0.7989311218261719\n",
      "Step [203/260]:\tLoss: 0.9674571752548218\n",
      "Step [204/260]:\tLoss: 0.858188271522522\n",
      "Step [205/260]:\tLoss: 0.8425256609916687\n",
      "Step [206/260]:\tLoss: 0.9079000949859619\n",
      "Step [207/260]:\tLoss: 0.887042224407196\n",
      "Step [208/260]:\tLoss: 0.8996938467025757\n",
      "Step [209/260]:\tLoss: 0.9534747004508972\n",
      "Step [210/260]:\tLoss: 0.7631997466087341\n",
      "Step [211/260]:\tLoss: 0.7177995443344116\n",
      "Step [212/260]:\tLoss: 0.9482037425041199\n",
      "Step [213/260]:\tLoss: 0.8231383562088013\n",
      "Step [214/260]:\tLoss: 0.763791024684906\n",
      "Step [215/260]:\tLoss: 0.8866060972213745\n",
      "Step [216/260]:\tLoss: 0.9807014465332031\n",
      "Step [217/260]:\tLoss: 0.8908811807632446\n",
      "Step [218/260]:\tLoss: 0.9016684293746948\n",
      "Step [219/260]:\tLoss: 0.7872962951660156\n",
      "Step [220/260]:\tLoss: 0.9022797346115112\n",
      "Step [221/260]:\tLoss: 0.8740043640136719\n",
      "Step [222/260]:\tLoss: 0.7646074295043945\n",
      "Step [223/260]:\tLoss: 0.9128173589706421\n",
      "Step [224/260]:\tLoss: 0.9404431581497192\n",
      "Step [225/260]:\tLoss: 0.827314019203186\n",
      "Step [226/260]:\tLoss: 0.8418818712234497\n",
      "Step [227/260]:\tLoss: 0.7874587774276733\n",
      "Step [228/260]:\tLoss: 0.9364922046661377\n",
      "Step [229/260]:\tLoss: 0.8672351837158203\n",
      "Step [230/260]:\tLoss: 0.8950262069702148\n",
      "Step [231/260]:\tLoss: 0.8951475024223328\n",
      "Step [232/260]:\tLoss: 0.8788245916366577\n",
      "Step [233/260]:\tLoss: 0.8633249998092651\n",
      "Step [234/260]:\tLoss: 0.7982279658317566\n",
      "Step [235/260]:\tLoss: 0.874146580696106\n",
      "Step [236/260]:\tLoss: 0.8777029514312744\n",
      "Step [237/260]:\tLoss: 0.9028905630111694\n",
      "Step [238/260]:\tLoss: 0.8998099565505981\n",
      "Step [239/260]:\tLoss: 0.7769762873649597\n",
      "Step [240/260]:\tLoss: 0.9178123474121094\n",
      "Step [241/260]:\tLoss: 0.8987468481063843\n",
      "Step [242/260]:\tLoss: 0.8446600437164307\n",
      "Step [243/260]:\tLoss: 0.9448168277740479\n",
      "Step [244/260]:\tLoss: 0.8791184425354004\n",
      "Step [245/260]:\tLoss: 0.9082901477813721\n",
      "Step [246/260]:\tLoss: 0.7669923305511475\n",
      "Step [247/260]:\tLoss: 0.7711191177368164\n",
      "Step [248/260]:\tLoss: 0.8456430435180664\n",
      "Step [249/260]:\tLoss: 0.8803638219833374\n",
      "Step [250/260]:\tLoss: 0.7932154536247253\n",
      "Step [251/260]:\tLoss: 0.797429084777832\n",
      "Step [252/260]:\tLoss: 0.9135556221008301\n",
      "Step [253/260]:\tLoss: 0.8026720285415649\n",
      "Step [254/260]:\tLoss: 0.7730728983879089\n",
      "Step [255/260]:\tLoss: 0.8048539161682129\n",
      "Step [256/260]:\tLoss: 0.9379504919052124\n",
      "Step [257/260]:\tLoss: 0.8658852577209473\n",
      "Step [258/260]:\tLoss: 0.9397973418235779\n",
      "Step [259/260]:\tLoss: 0.8971884250640869\n",
      "Epoch [1/1000]: Loss/train: 0.9609671934292867\ttime:31.705262899398804\n",
      "Step [0/260]:\tLoss: 0.776885986328125\n",
      "Step [1/260]:\tLoss: 0.9008661508560181\n",
      "Step [2/260]:\tLoss: 0.7761175036430359\n",
      "Step [3/260]:\tLoss: 0.840621829032898\n",
      "Step [4/260]:\tLoss: 0.8181252479553223\n",
      "Step [5/260]:\tLoss: 0.903329074382782\n",
      "Step [6/260]:\tLoss: 0.9392907023429871\n",
      "Step [7/260]:\tLoss: 0.7582920789718628\n",
      "Step [8/260]:\tLoss: 0.9776689410209656\n",
      "Step [9/260]:\tLoss: 1.054077386856079\n",
      "Step [10/260]:\tLoss: 0.9122300148010254\n",
      "Step [11/260]:\tLoss: 0.8511937856674194\n",
      "Step [12/260]:\tLoss: 0.8102972507476807\n",
      "Step [13/260]:\tLoss: 0.8103012442588806\n",
      "Step [14/260]:\tLoss: 0.9442469477653503\n",
      "Step [15/260]:\tLoss: 0.7695114016532898\n",
      "Step [16/260]:\tLoss: 0.8622469902038574\n",
      "Step [17/260]:\tLoss: 0.8722772598266602\n",
      "Step [18/260]:\tLoss: 0.7426553964614868\n",
      "Step [19/260]:\tLoss: 0.9212402105331421\n",
      "Step [20/260]:\tLoss: 0.8215174674987793\n",
      "Step [21/260]:\tLoss: 1.061277151107788\n",
      "Step [22/260]:\tLoss: 0.7780827283859253\n",
      "Step [23/260]:\tLoss: 0.8621805906295776\n",
      "Step [24/260]:\tLoss: 0.8829406499862671\n",
      "Step [25/260]:\tLoss: 0.7551827430725098\n",
      "Step [26/260]:\tLoss: 0.8800577521324158\n",
      "Step [27/260]:\tLoss: 0.9094842672348022\n",
      "Step [28/260]:\tLoss: 0.7533490061759949\n",
      "Step [29/260]:\tLoss: 0.7705197334289551\n",
      "Step [30/260]:\tLoss: 0.9419478178024292\n",
      "Step [31/260]:\tLoss: 0.8794366121292114\n",
      "Step [32/260]:\tLoss: 0.9342691898345947\n",
      "Step [33/260]:\tLoss: 0.8370155096054077\n",
      "Step [34/260]:\tLoss: 0.8858530521392822\n",
      "Step [35/260]:\tLoss: 0.8199752569198608\n",
      "Step [36/260]:\tLoss: 0.8489155769348145\n",
      "Step [37/260]:\tLoss: 0.8369542360305786\n",
      "Step [38/260]:\tLoss: 0.8366353511810303\n",
      "Step [39/260]:\tLoss: 0.8162140846252441\n",
      "Step [40/260]:\tLoss: 0.8000779151916504\n",
      "Step [41/260]:\tLoss: 0.7876229286193848\n",
      "Step [42/260]:\tLoss: 0.9562487602233887\n",
      "Step [43/260]:\tLoss: 0.8685290217399597\n",
      "Step [44/260]:\tLoss: 0.8255845904350281\n",
      "Step [45/260]:\tLoss: 0.8966474533081055\n",
      "Step [46/260]:\tLoss: 0.8296407461166382\n",
      "Step [47/260]:\tLoss: 0.738585352897644\n",
      "Step [48/260]:\tLoss: 0.8417171239852905\n",
      "Step [49/260]:\tLoss: 0.805008590221405\n",
      "Step [50/260]:\tLoss: 0.8290995359420776\n",
      "Step [51/260]:\tLoss: 0.8869305849075317\n",
      "Step [52/260]:\tLoss: 0.7992233633995056\n",
      "Step [53/260]:\tLoss: 0.8720930814743042\n",
      "Step [54/260]:\tLoss: 0.9108855128288269\n",
      "Step [55/260]:\tLoss: 0.8358342051506042\n",
      "Step [56/260]:\tLoss: 0.8983268737792969\n",
      "Step [57/260]:\tLoss: 0.840448260307312\n",
      "Step [58/260]:\tLoss: 0.8125625848770142\n",
      "Step [59/260]:\tLoss: 0.8403503894805908\n",
      "Step [60/260]:\tLoss: 0.8561196327209473\n",
      "Step [61/260]:\tLoss: 0.9121989011764526\n",
      "Step [62/260]:\tLoss: 0.8633521795272827\n",
      "Step [63/260]:\tLoss: 0.764638364315033\n",
      "Step [64/260]:\tLoss: 0.8544638156890869\n",
      "Step [65/260]:\tLoss: 0.8640528917312622\n",
      "Step [66/260]:\tLoss: 0.8891026377677917\n",
      "Step [67/260]:\tLoss: 0.8465530276298523\n",
      "Step [68/260]:\tLoss: 0.7200207710266113\n",
      "Step [69/260]:\tLoss: 0.971389651298523\n",
      "Step [70/260]:\tLoss: 0.8203966617584229\n",
      "Step [71/260]:\tLoss: 0.7699443697929382\n",
      "Step [72/260]:\tLoss: 0.8020843863487244\n",
      "Step [73/260]:\tLoss: 0.7972144484519958\n",
      "Step [74/260]:\tLoss: 0.8561145067214966\n",
      "Step [75/260]:\tLoss: 0.8513433337211609\n",
      "Step [76/260]:\tLoss: 0.7381575107574463\n",
      "Step [77/260]:\tLoss: 0.8141843676567078\n",
      "Step [78/260]:\tLoss: 0.9446344375610352\n",
      "Step [79/260]:\tLoss: 0.9493645429611206\n",
      "Step [80/260]:\tLoss: 0.8665854334831238\n",
      "Step [81/260]:\tLoss: 0.7550721168518066\n",
      "Step [82/260]:\tLoss: 0.9186879992485046\n",
      "Step [83/260]:\tLoss: 0.8504185676574707\n",
      "Step [84/260]:\tLoss: 0.9153207540512085\n",
      "Step [85/260]:\tLoss: 0.8611182570457458\n",
      "Step [86/260]:\tLoss: 0.9287402629852295\n",
      "Step [87/260]:\tLoss: 0.985185980796814\n",
      "Step [88/260]:\tLoss: 0.8466386795043945\n",
      "Step [89/260]:\tLoss: 1.0093238353729248\n",
      "Step [90/260]:\tLoss: 0.8672122359275818\n",
      "Step [91/260]:\tLoss: 0.793193519115448\n",
      "Step [92/260]:\tLoss: 0.9040864706039429\n",
      "Step [93/260]:\tLoss: 0.9311448931694031\n",
      "Step [94/260]:\tLoss: 0.8322619199752808\n",
      "Step [95/260]:\tLoss: 0.8270766139030457\n",
      "Step [96/260]:\tLoss: 0.8165170550346375\n",
      "Step [97/260]:\tLoss: 0.9172838926315308\n",
      "Step [98/260]:\tLoss: 0.7925119400024414\n",
      "Step [99/260]:\tLoss: 0.8861989378929138\n",
      "Step [100/260]:\tLoss: 0.8013739585876465\n",
      "Step [101/260]:\tLoss: 0.822361946105957\n",
      "Step [102/260]:\tLoss: 0.9166350364685059\n",
      "Step [103/260]:\tLoss: 0.8992282748222351\n",
      "Step [104/260]:\tLoss: 0.7992233037948608\n",
      "Step [105/260]:\tLoss: 0.8697335124015808\n",
      "Step [106/260]:\tLoss: 0.8088533878326416\n",
      "Step [107/260]:\tLoss: 0.7843007445335388\n",
      "Step [108/260]:\tLoss: 0.8627885580062866\n",
      "Step [109/260]:\tLoss: 0.9326149821281433\n",
      "Step [110/260]:\tLoss: 0.8541883230209351\n",
      "Step [111/260]:\tLoss: 0.8013955950737\n",
      "Step [112/260]:\tLoss: 0.8607720136642456\n",
      "Step [113/260]:\tLoss: 0.7856401205062866\n",
      "Step [114/260]:\tLoss: 0.9049936532974243\n",
      "Step [115/260]:\tLoss: 0.8399776816368103\n",
      "Step [116/260]:\tLoss: 0.9007522463798523\n",
      "Step [117/260]:\tLoss: 0.7605172991752625\n",
      "Step [118/260]:\tLoss: 0.7675713300704956\n",
      "Step [119/260]:\tLoss: 0.7678343653678894\n",
      "Step [120/260]:\tLoss: 0.830764889717102\n",
      "Step [121/260]:\tLoss: 0.9440789222717285\n",
      "Step [122/260]:\tLoss: 0.8843976855278015\n",
      "Step [123/260]:\tLoss: 0.9089770317077637\n",
      "Step [124/260]:\tLoss: 0.8407714366912842\n",
      "Step [125/260]:\tLoss: 0.8555004000663757\n",
      "Step [126/260]:\tLoss: 0.8234285116195679\n",
      "Step [127/260]:\tLoss: 0.9654983878135681\n",
      "Step [128/260]:\tLoss: 0.8197011947631836\n",
      "Step [129/260]:\tLoss: 0.8889302611351013\n",
      "Step [130/260]:\tLoss: 0.9159339666366577\n",
      "Step [131/260]:\tLoss: 0.8770623207092285\n",
      "Step [132/260]:\tLoss: 0.8072834014892578\n",
      "Step [133/260]:\tLoss: 0.8188403844833374\n",
      "Step [134/260]:\tLoss: 0.8251931071281433\n",
      "Step [135/260]:\tLoss: 0.9194512367248535\n",
      "Step [136/260]:\tLoss: 0.8669061660766602\n",
      "Step [137/260]:\tLoss: 0.9451645612716675\n",
      "Step [138/260]:\tLoss: 0.7959715127944946\n",
      "Step [139/260]:\tLoss: 0.8787699937820435\n",
      "Step [140/260]:\tLoss: 0.8157347440719604\n",
      "Step [141/260]:\tLoss: 0.9055653810501099\n",
      "Step [142/260]:\tLoss: 0.8787145018577576\n",
      "Step [143/260]:\tLoss: 0.7755672335624695\n",
      "Step [144/260]:\tLoss: 0.8126323223114014\n",
      "Step [145/260]:\tLoss: 0.83965003490448\n",
      "Step [146/260]:\tLoss: 0.873418927192688\n",
      "Step [147/260]:\tLoss: 0.8239808082580566\n",
      "Step [148/260]:\tLoss: 0.8394044637680054\n",
      "Step [149/260]:\tLoss: 0.8294646143913269\n",
      "Step [150/260]:\tLoss: 0.7951343059539795\n",
      "Step [151/260]:\tLoss: 0.8850835561752319\n",
      "Step [152/260]:\tLoss: 0.8712042570114136\n",
      "Step [153/260]:\tLoss: 0.7946736216545105\n",
      "Step [154/260]:\tLoss: 0.8175076842308044\n",
      "Step [155/260]:\tLoss: 0.7617291212081909\n",
      "Step [156/260]:\tLoss: 0.9077789783477783\n",
      "Step [157/260]:\tLoss: 0.8178224563598633\n",
      "Step [158/260]:\tLoss: 0.8085893392562866\n",
      "Step [159/260]:\tLoss: 0.7900916934013367\n",
      "Step [160/260]:\tLoss: 0.8592925071716309\n",
      "Step [161/260]:\tLoss: 0.7556809186935425\n",
      "Step [162/260]:\tLoss: 0.8680847883224487\n",
      "Step [163/260]:\tLoss: 0.7772327661514282\n",
      "Step [164/260]:\tLoss: 0.825326144695282\n",
      "Step [165/260]:\tLoss: 0.917194128036499\n",
      "Step [166/260]:\tLoss: 0.8171490430831909\n",
      "Step [167/260]:\tLoss: 0.7410140037536621\n",
      "Step [168/260]:\tLoss: 0.7732264399528503\n",
      "Step [169/260]:\tLoss: 0.9062714576721191\n",
      "Step [170/260]:\tLoss: 0.9414461851119995\n",
      "Step [171/260]:\tLoss: 0.8329113721847534\n",
      "Step [172/260]:\tLoss: 0.7580406665802002\n",
      "Step [173/260]:\tLoss: 0.8550402522087097\n",
      "Step [174/260]:\tLoss: 0.8659579157829285\n",
      "Step [175/260]:\tLoss: 0.9460851550102234\n",
      "Step [176/260]:\tLoss: 0.8437199592590332\n",
      "Step [177/260]:\tLoss: 0.9446887373924255\n",
      "Step [178/260]:\tLoss: 0.7902512550354004\n",
      "Step [179/260]:\tLoss: 0.8072606921195984\n",
      "Step [180/260]:\tLoss: 0.7537072896957397\n",
      "Step [181/260]:\tLoss: 0.7679734230041504\n",
      "Step [182/260]:\tLoss: 0.7588507533073425\n",
      "Step [183/260]:\tLoss: 0.7687796354293823\n",
      "Step [184/260]:\tLoss: 0.8387327194213867\n",
      "Step [185/260]:\tLoss: 0.7822637557983398\n",
      "Step [186/260]:\tLoss: 0.8265933990478516\n",
      "Step [187/260]:\tLoss: 0.8767420649528503\n",
      "Step [188/260]:\tLoss: 0.8395804166793823\n",
      "Step [189/260]:\tLoss: 0.7588350772857666\n",
      "Step [190/260]:\tLoss: 0.8462249636650085\n",
      "Step [191/260]:\tLoss: 0.6611389517784119\n",
      "Step [192/260]:\tLoss: 0.7727169990539551\n",
      "Step [193/260]:\tLoss: 0.940872073173523\n",
      "Step [194/260]:\tLoss: 0.8581124544143677\n",
      "Step [195/260]:\tLoss: 0.7734758257865906\n",
      "Step [196/260]:\tLoss: 0.7702090740203857\n",
      "Step [197/260]:\tLoss: 0.8012034296989441\n",
      "Step [198/260]:\tLoss: 0.8507367968559265\n",
      "Step [199/260]:\tLoss: 0.8156134486198425\n",
      "Step [200/260]:\tLoss: 0.7064285278320312\n",
      "Step [201/260]:\tLoss: 0.6813335418701172\n",
      "Step [202/260]:\tLoss: 0.8305932879447937\n",
      "Step [203/260]:\tLoss: 0.7615219950675964\n",
      "Step [204/260]:\tLoss: 0.8687297701835632\n",
      "Step [205/260]:\tLoss: 0.9356621503829956\n",
      "Step [206/260]:\tLoss: 0.8793596029281616\n",
      "Step [207/260]:\tLoss: 0.8132607340812683\n",
      "Step [208/260]:\tLoss: 0.8008993864059448\n",
      "Step [209/260]:\tLoss: 0.8887969851493835\n",
      "Step [210/260]:\tLoss: 0.8064353466033936\n",
      "Step [211/260]:\tLoss: 0.8404109477996826\n",
      "Step [212/260]:\tLoss: 0.7240719795227051\n",
      "Step [213/260]:\tLoss: 0.7642602920532227\n",
      "Step [214/260]:\tLoss: 0.6526514887809753\n",
      "Step [215/260]:\tLoss: 0.7428114414215088\n",
      "Step [216/260]:\tLoss: 0.7591423392295837\n",
      "Step [217/260]:\tLoss: 0.7802079916000366\n",
      "Step [218/260]:\tLoss: 0.7964035868644714\n",
      "Step [219/260]:\tLoss: 0.6736497282981873\n",
      "Step [220/260]:\tLoss: 0.8719951510429382\n",
      "Step [221/260]:\tLoss: 0.8340908885002136\n",
      "Step [222/260]:\tLoss: 0.6534827947616577\n",
      "Step [223/260]:\tLoss: 0.8395166397094727\n",
      "Step [224/260]:\tLoss: 0.8032814860343933\n",
      "Step [225/260]:\tLoss: 0.7953389883041382\n",
      "Step [226/260]:\tLoss: 0.8586993217468262\n",
      "Step [227/260]:\tLoss: 0.9291128516197205\n",
      "Step [228/260]:\tLoss: 0.9033058881759644\n",
      "Step [229/260]:\tLoss: 0.7771047353744507\n",
      "Step [230/260]:\tLoss: 0.8732265830039978\n",
      "Step [231/260]:\tLoss: 0.9162017107009888\n",
      "Step [232/260]:\tLoss: 0.8123564720153809\n",
      "Step [233/260]:\tLoss: 0.7799240946769714\n",
      "Step [234/260]:\tLoss: 0.8359627723693848\n",
      "Step [235/260]:\tLoss: 0.8810415267944336\n",
      "Step [236/260]:\tLoss: 0.8324486017227173\n",
      "Step [237/260]:\tLoss: 0.7440301179885864\n",
      "Step [238/260]:\tLoss: 0.8026115298271179\n",
      "Step [239/260]:\tLoss: 0.8035421371459961\n",
      "Step [240/260]:\tLoss: 0.7905721664428711\n",
      "Step [241/260]:\tLoss: 0.7725388407707214\n",
      "Step [242/260]:\tLoss: 0.8343878984451294\n",
      "Step [243/260]:\tLoss: 0.8219403624534607\n",
      "Step [244/260]:\tLoss: 0.7555978298187256\n",
      "Step [245/260]:\tLoss: 0.8196943998336792\n",
      "Step [246/260]:\tLoss: 0.861153244972229\n",
      "Step [247/260]:\tLoss: 0.8387400507926941\n",
      "Step [248/260]:\tLoss: 0.7132918238639832\n",
      "Step [249/260]:\tLoss: 0.8650164008140564\n",
      "Step [250/260]:\tLoss: 0.8765835762023926\n",
      "Step [251/260]:\tLoss: 0.6784514784812927\n",
      "Step [252/260]:\tLoss: 0.8142797350883484\n",
      "Step [253/260]:\tLoss: 0.728843092918396\n",
      "Step [254/260]:\tLoss: 0.761042594909668\n",
      "Step [255/260]:\tLoss: 0.8355533480644226\n",
      "Step [256/260]:\tLoss: 0.75096195936203\n",
      "Step [257/260]:\tLoss: 0.7193788290023804\n",
      "Step [258/260]:\tLoss: 0.7580221891403198\n",
      "Step [259/260]:\tLoss: 0.8263014554977417\n",
      "Epoch [2/1000]: Loss/train: 0.8360778240057138\ttime:32.15497612953186\n",
      "Step [0/260]:\tLoss: 0.7857787013053894\n",
      "Step [1/260]:\tLoss: 0.8502068519592285\n",
      "Step [2/260]:\tLoss: 0.7235697507858276\n",
      "Step [3/260]:\tLoss: 0.7815850973129272\n",
      "Step [4/260]:\tLoss: 0.9135032892227173\n",
      "Step [5/260]:\tLoss: 0.8644463419914246\n",
      "Step [6/260]:\tLoss: 0.8491289615631104\n",
      "Step [7/260]:\tLoss: 0.7349419593811035\n",
      "Step [8/260]:\tLoss: 0.8809976577758789\n",
      "Step [9/260]:\tLoss: 0.8567458391189575\n",
      "Step [10/260]:\tLoss: 0.8826262354850769\n",
      "Step [11/260]:\tLoss: 0.9101834297180176\n",
      "Step [12/260]:\tLoss: 0.7986993193626404\n",
      "Step [13/260]:\tLoss: 0.8051683306694031\n",
      "Step [14/260]:\tLoss: 0.8460038900375366\n",
      "Step [15/260]:\tLoss: 0.8108431696891785\n",
      "Step [16/260]:\tLoss: 0.7553696036338806\n",
      "Step [17/260]:\tLoss: 0.7954601049423218\n",
      "Step [18/260]:\tLoss: 0.8106247186660767\n",
      "Step [19/260]:\tLoss: 0.7830986976623535\n",
      "Step [20/260]:\tLoss: 0.8297019004821777\n",
      "Step [21/260]:\tLoss: 0.7923883199691772\n",
      "Step [22/260]:\tLoss: 0.9226837158203125\n",
      "Step [23/260]:\tLoss: 0.8110639452934265\n",
      "Step [24/260]:\tLoss: 0.7858144640922546\n",
      "Step [25/260]:\tLoss: 0.7886685132980347\n",
      "Step [26/260]:\tLoss: 0.803853452205658\n",
      "Step [27/260]:\tLoss: 0.9057822227478027\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.cuda) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "resnet = ResNet18()\n",
    "\n",
    "model = BYOL(resnet, image_size=args.image_size, hidden_layer=\"avgpool\")\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# solver\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.num_epochs+1):\n",
    "    metrics = defaultdict(list)\n",
    "    start = time.time()\n",
    "    \n",
    "    with open('log_train_loss.txt', 'a') as f:\n",
    "        f.write(f\"{epoch}Epoch start\\n\")\n",
    "    \n",
    "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "        x_i = x_i.to(device)\n",
    "        x_j = x_j.to(device)\n",
    "\n",
    "        loss = model(x_i, x_j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.update_moving_average()  # update moving average of target encoder\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            with open('log_train_loss.txt', 'a') as f:\n",
    "                f.write(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\\n\")\n",
    "            #print(f\"Step [{step}/{len(train_loader)}]:\\tLoss: {loss.item()}\")\n",
    "\n",
    "        metrics[\"Loss/train\"].append(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "    # write metrics to TensorBoard\n",
    "    epoch_time = time.time() - start\n",
    "    print(f\"Epoch [{epoch}/{args.num_epochs}]: \" + \"\\t\".join([f\"{k}: {np.array(v).mean()}\" for k, v in metrics.items()])  + f\"\\ttime:{epoch_time}\")\n",
    "    \n",
    "    with open('log_train_epoch_loss.txt', 'a') as f:\n",
    "        f.write(f\"Epoch [{epoch}/{args.num_epochs}]: \" + \"\\t\".join([f\"{k}: {np.array(v).mean()}\" for k, v in metrics.items()])  + f\"\\ttime:{epoch_time}\\n\")\n",
    "\n",
    "    if epoch % args.checkpoint_epochs == 0:\n",
    "        print(f\"Saving model at epoch {epoch}\")\n",
    "        torch.save(resnet.state_dict(), f\"./model-{epoch}.pt\")\n",
    "\n",
    "# save your improved network\n",
    "torch.save(resnet.state_dict(), \"./model-final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482186cf-f703-48b0-9b9e-e835ea970da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae248961-1dfc-4394-a8dc-7ddf6b87be1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303b969-9cf8-41c5-b18f-12b932963fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(loader, model, device):\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # get encoding\n",
    "        with torch.no_grad():\n",
    "            h = model(x)\n",
    "\n",
    "        h = h.squeeze()\n",
    "        h = h.detach()\n",
    "\n",
    "        feature_vector.extend(h.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "\n",
    "        # if step % 5 == 0:\n",
    "        #     print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    # print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    return feature_vector, labels_vector\n",
    "\n",
    "def get_features(model, train_loader, test_loader, device):\n",
    "    train_X, train_y = inference(train_loader, model, device)\n",
    "    test_X, test_y = inference(test_loader, model, device)\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "\n",
    "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test, batch_size):\n",
    "    train = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288da08e-a23c-45ab-a67a-d7c2bd6ed5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### parameter設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22518b4a-50cb-46f2-9c64-164603d89868",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--image_size\", default=32, type=int, help=\"Image size\")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\", default=3e-3, type=float, help=\"Initial learning rate.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=768, type=int, help=\"Batch size for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\", default=300, type=int, help=\"Number of epochs to linear train for.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--saved_checkpoint_epochs\", default=50, type=int, help=\"checkpoint epoch set at the time of learning\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--saved_epoch\", default=1000, type=int, help=\"Saved model max epoch.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_epochs\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"Number of epochs between checkpoints/summaries.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_dir\",\n",
    "    default=\"./datasets\",\n",
    "    type=str,\n",
    "    help=\"Directory where dataset is stored.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    default=8,\n",
    "    type=int,\n",
    "    help=\"Number of data loading workers (caution with nodes!)\",\n",
    ")\n",
    "parser.add_argument(\"--cuda\", default=\"cuda:0\", type=str, help=\"cuda number\")\n",
    "parser.add_argument(\"--seed\", default=0, type=int, help=\"number of seed\")\n",
    "parser.add_argument(\"--k\", default=15, type=int, help=\"k of knn\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.model_path = \"model-final.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17886c19-88a2-491d-b4d6-ca03105eed80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285d145-ccba-4f2b-a4b9-d064b6db33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    args.dataset_dir,\n",
    "    download=True,\n",
    "    transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    args.dataset_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=TransformsSimCLR(size=args.image_size).test_transform,\n",
    ")\n",
    "\n",
    "Train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    ")\n",
    "\n",
    "Test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    num_workers=args.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c0fa6-e8df-4a54-9313-620f81e18d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 線形分類器とknnの学習と評価\n",
    "modelの学習時に保存したモデルの数読み込む。<br>\n",
    "例）学習時に1000epoch回して、50epochごとに保存したならば20個のモデルが存在するので、20回読み込み評価する。(変更したいのならargsを変える)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3955d-a0c8-44c4-b236-070921c671dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading features ###\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.cuda) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "epoch_linear_test_acc = []\n",
    "epoch_knn_test_acc = []\n",
    "result_df = pd.DataFrame(columns=[\"Linear Acc\", \"K-NN Acc\"])\n",
    "\n",
    "num_models = args.saved_epoch // args.saved_checkpoint_epochs\n",
    "\n",
    "for i in tqdm(range(num_models+1)):\n",
    "    # pre-trained model\n",
    "    resnet = ResNet18()\n",
    "    resnet.load_state_dict(torch.load(f\"model-{i*args.saved_checkpoint_epochs}.pt\", map_location=device))\n",
    "    resnet = resnet.to(device)\n",
    "\n",
    "    num_features = list(resnet.children())[-1].in_features\n",
    "\n",
    "    # throw away fc layer\n",
    "    resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    n_classes = 10 # CIFAR-10 has 10 classes\n",
    "\n",
    "    # fine-tune model\n",
    "    logreg = nn.Sequential(nn.Linear(num_features, n_classes))\n",
    "    logreg = logreg.to(device)\n",
    "\n",
    "    # loss / optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=logreg.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    if not os.path.exists(f\"features_{i*args.saved_checkpoint_epochs}.p\"):\n",
    "        print(\"### Creating features from pre-trained model ###\")\n",
    "        (train_X, train_y, test_X, test_y) = get_features(\n",
    "            resnet, Train_loader, Test_loader, device\n",
    "        )\n",
    "        pickle.dump(\n",
    "            (train_X, train_y, test_X, test_y), open(f\"features_{i*args.saved_checkpoint_epochs}.p\", \"wb\"), protocol=4\n",
    "        )\n",
    "    else:\n",
    "        print(\"### Loading features ###\")\n",
    "        (train_X, train_y, test_X, test_y) = pickle.load(open(f\"features_{i*args.saved_checkpoint_epochs}.p\", \"rb\"))\n",
    "\n",
    "\n",
    "    train_loader, test_loader = create_data_loaders_from_arrays(\n",
    "        train_X, train_y, test_X, test_y, 2048 \n",
    "    )\n",
    "    \n",
    "    #eval_train----------------------------------------------------------------------------------------------\n",
    "    #linear\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for step, (h, y) in enumerate(train_loader):\n",
    "            h = h.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = logreg(h)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #accuracyの計算はぶく\n",
    "    #knn        \n",
    "    knn = KNeighborsClassifier(n_neighbors=args.k)\n",
    "    knn.fit(train_X, train_y)\n",
    "    #--------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #eval_test-----------------------------------------------------------------------------------------------\n",
    "    #linear\n",
    "    metrics = defaultdict(list)\n",
    "    for step, (h, y) in enumerate(test_loader):\n",
    "        h = h.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = logreg(h)\n",
    "\n",
    "        # calculate accuracy and save metrics\n",
    "        linear_acc = (outputs.argmax(1) == y).sum().item() / y.size(0)\n",
    "        metrics[\"Accuracy/linear\"].append(linear_acc)\n",
    "    #knn\n",
    "    pred_labels = knn.predict(test_X)\n",
    "    knn_acc = sum(pred_labels == test_y)/len(test_y)\n",
    "    #--------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #結果の表示と保存\n",
    "    print(f\"[{i*args.saved_checkpoint_epochs}epoch] \" + \"\\t\".join([f\"{k}: {np.array(v).mean():.4}\" for k, v in metrics.items()])\n",
    "         + \"\\t\"f\"Accuracy/knn: {knn_acc:.4}\")\n",
    "    \n",
    "    with open('log_test_acc.txt', 'a') as f:\n",
    "        f.write(f\"[{i*args.saved_checkpoint_epochs}epoch] \" + \"\\t\".join([f\"{k}: {np.array(v).mean():.4}\" for k, v in metrics.items()])\n",
    "         + \"\\t\"f\"Accuracy/knn: {knn_acc:.4}\\n\")\n",
    "    \n",
    "    result_df.loc[i*args.saved_checkpoint_epochs, \"Linear Acc\"] = np.array(metrics[\"Accuracy/linear\"]).mean()\n",
    "    result_df.loc[i*args.saved_checkpoint_epochs, \"K-NN Acc\"] = knn_acc\n",
    "\n",
    "    epoch_linear_test_acc.append(np.array(metrics[\"Accuracy/linear\"]).mean())\n",
    "    epoch_knn_test_acc.append(knn_acc)\n",
    "    \n",
    "result_df.to_csv(\"./result_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5d4417-5819-4610-a48f-892f9d288ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"BYOL 0-1000epoch Linear & knn eval\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(0, 1050, 50),epoch_linear_test_acc, color='red',linestyle='-', label=\"linear\")\n",
    "plt.plot(range(0, 1050, 50),epoch_knn_test_acc, color='blue',linestyle='-', label=\"knn\")\n",
    "plt.legend()\n",
    "plt.savefig(\"BYOL 0-1000epoch Linear & knn eval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddf37d1e-38ee-44ad-88b9-9db5dd0fd98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear Acc</th>\n",
       "      <th>K-NN Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.448479</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769545</td>\n",
       "      <td>0.729768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810296</td>\n",
       "      <td>0.783353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.835226</td>\n",
       "      <td>0.812300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.843248</td>\n",
       "      <td>0.825721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.851242</td>\n",
       "      <td>0.835236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.858426</td>\n",
       "      <td>0.841647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.863616</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.865695</td>\n",
       "      <td>0.845954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.863518</td>\n",
       "      <td>0.847256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.868318</td>\n",
       "      <td>0.848858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.869127</td>\n",
       "      <td>0.850461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.874107</td>\n",
       "      <td>0.851663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.872503</td>\n",
       "      <td>0.849860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.870006</td>\n",
       "      <td>0.852664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.871805</td>\n",
       "      <td>0.852664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.875321</td>\n",
       "      <td>0.854267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.872810</td>\n",
       "      <td>0.850260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.873689</td>\n",
       "      <td>0.851863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.875767</td>\n",
       "      <td>0.853766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.868722</td>\n",
       "      <td>0.849559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Linear Acc  K-NN Acc\n",
       "0     0.448479  0.354167\n",
       "1     0.769545  0.729768\n",
       "2     0.810296  0.783353\n",
       "3     0.835226  0.812300\n",
       "4     0.843248  0.825721\n",
       "5     0.851242  0.835236\n",
       "6     0.858426  0.841647\n",
       "7     0.863616  0.846154\n",
       "8     0.865695  0.845954\n",
       "9     0.863518  0.847256\n",
       "10    0.868318  0.848858\n",
       "11    0.869127  0.850461\n",
       "12    0.874107  0.851663\n",
       "13    0.872503  0.849860\n",
       "14    0.870006  0.852664\n",
       "15    0.871805  0.852664\n",
       "16    0.875321  0.854267\n",
       "17    0.872810  0.850260\n",
       "18    0.873689  0.851863\n",
       "19    0.875767  0.853766\n",
       "20    0.868722  0.849559"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./result_df.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca9ceb-c294-4824-87b6-71f9720b61bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
